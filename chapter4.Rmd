#Clusters and Classification
This is the Clustering and Classification homework. We'll start by taking a gander at the data.
```{r}
library(MASS)
data("Boston")
str(Boston);View(Boston)
```
  
  These variable names tell us little about what they represent. I'll just copypaste the descriptions from the original [website](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html):


crim

    per capita crime rate by town.
zn

    proportion of residential land zoned for lots over 25,000 sq.ft.
indus

    proportion of non-retail business acres per town.
chas

    Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
nox

    nitrogen oxides concentration (parts per 10 million).
rm

    average number of rooms per dwelling.
age

    proportion of owner-occupied units built prior to 1940.
dis

    weighted mean of distances to five Boston employment centres.
rad

    index of accessibility to radial highways.
tax

    full-value property-tax rate per \$10,000.
ptratio

    pupil-teacher ratio by town.
black

    1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.
lstat

    lower status of the population (percent).
medv

    median value of owner-occupied homes in \$1000s.

    
Not even the host page tells us much about the purpose of this dataset. If I had to guess, I'd guess it was collected to analyze correlates and predictors of house prices. Here are the summaries:
```{r}
summary(Boston)
```
~~Because Linear Discriminant Analysis is linear, it requires the explanatory variables to be continuous. That is doubtless why this dataset was chosen for this chapter -- there are only numeric variables.~~ Despite the use of the word _linear_, LDA does NOT require the predictor variables to be linear. It only requires that they can be represented numerically. Even our Boston dataset contains one binary variable, namely _chas_. Since LDA apparently places no requirements on the nature of the predictor variables, it is quite a versatile statistical tool indeed. What it does require is that the predictors be normally distributed and have the same variance. However, according to [this page](http://machinelearningmastery.com/linear-discriminant-analysis-for-machine-learning/), there are valid work-arounds even when these requirements are not met.
  
  We will now proceed to draw a plot of the correlations of each pair of variables:
```{r}
library("corrplot");library("tidyverse")
(matriisi<-cor(Boston)) %>% corrplot(method="circle")
```
  
  Circle diameter and darkness illustrate the strength of the variable's correlation with the variable that intersects with it in the matrix. Blue color means a positive correlation, red is negative. The strongest correlations that catch my eye in this plot are:
  
  * Air pollution levels and distance from large employment centers (negative). This is a 'well, duh' observation -- of course pollution is lower the farther you get from the chimneys and smokestacks.
  * Property tax and the amount of industry in town -- this correlation is not at all self-evident to me. I would have assumed that taxes are lower in industrial centers because industrious people create wealth and don't need income redistribution in the form of high taxes. But I seem to be missing something critical.
  * Distance from emplyoment centers is also strongly negatively correlated with the amount of industry in town. I guess this means people don't want to run their factories in remote locations where the long commute would turn off potential employees.
  
Now it's time to scale the data.

```{r}
Boston.unscaled<-Boston #Backing up the unscaled version
Boston<-scale(Boston)
summary(Boston)
```

Now every variable has been standardized and centered by subtracting the mean from the value and dividing the remainder by the standard deviation of that variable. It makes comparisons of variance much easier and more intuitive. Next we convert _crim_(e rate) into a categorical variable AKA factor. This time we also do what is seldom advisable, i.e., delete the old, numeric variable afterwards:

```{r}
Boston<-as.data.frame(Boston)
(kvantiilit<-quantile(Boston$crim))
Boston$crime.factor<-cut(Boston$crim, kvantiilit, include.lowest = TRUE, labels=c("low","low-ish","high-ish","high"))
Boston<-Boston[,-1]
```
The __scale()__ function turned _Boston_ into a matrix object, which doesn't behave quite the way we want here, so I had to convert it back into a data frame before being able to proceed. Don't ask why __scale()__ does that.

Now we must randomly split the data into a training set consisting of 80% of the observations and a testing set comprising the remaining 20%:

```{r}
set.seed(10)
TrainingRows<-sample(nrow(Boston),size=nrow(Boston)*.8) #Randomly select 80% of all rows
TrainingSet<-Boston[TrainingRows,] #Create training set
TestingSet<-Boston[-TrainingRows,] #Create testing set
head(TrainingSet)
head(TestingSet)
```
##Linear Discriminant Analysis  
So now we will follow a procedure very similar to the _Cross-Validation_ seen in the previous chapter. We will first build a model on the training dataset, then we will test its predictive power on the testing dataset. A new, useful detail is that if you want to use all the variables found in the data frame as predictors, you can use a single dot as a wildcard for that:
  
```{r}
(lda.fit<-lda(crime.factor ~ ., data = TrainingSet))
```
  
  Linear Discriminant Analysis is a pattern-recognition and dimensionality-reduction technique. It is used to to identify patterns in the behavior of a set of continuous explanatory variables in 2 or more known categories/groups (the dependent variable), and distill these different behaviors into a set of "discriminants" that are specific combinations of values that the explanatory variables tend to take in each category/group.   
    
  For instance, imagine you're an entrepreneur selling a single product that you've worked years on. To improve the success rate of your marketing efforts, you might gather all the available information on the people you've ever solicited in your career, and conduct a LDA to identify what the people who actually buy from you have in common. Say you're able to access the information on the following set of explanatory variables: age, income level, BMI, number of children, hair length, size of current home town -- that's 7 explanatory variables. Conducting an LDA might reveal that the people who buy from you tend to be over the age of 30, have more than 3 kids, live in very small towns, and have medium-to-low income. That is their "discriminant", and you can multiply your sales by focusing your marketing on people who match that profile.  
  
  In the exercise at hand, we're trying to identify the discriminants of the 4 different levels of crime rate. I'm not particularly enamored with this categorization: I view this dependent variable as essentially continuous, and its conversion into a factor seems somewhat artificial to me. But the instructions are what they are, and obey we must. Here's an arrow bi-plot of the LDA:
  
```{r}
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}
classes <- as.numeric(TrainingSet$crime.factor)
plot(lda.fit, dimen = 2, col=classes, pch=classes);lda.arrows(lda.fit, myscale = 1.5)
```
  
  Frankly, I don't get much out of this plot. It's too messy and cluttered for me to fully wrap my head around.  
    
  Our next step is using the model for predictions. We'll start by storing the actual crime categories of the testing set in a separate vector and then removing that column from the dataset, in order to make the prediction game more interesting:
    
```{r}
correct.classes<-TestingSet$crime.factor
TestingSet<-TestingSet[,which(colnames(TestingSet)!="crime.factor")]
```  
  
  Then we make predictions in a familiar way...
  
```{r}
lda.pred <- predict(lda.fit, newdata = TestingSet)
```  
  
  ...followed by a cross-tabulation of the predicted and observed outcomes:

```{r}
table(real = correct.classes, predicted = lda.pred$class)
```

This shows us that the model does well at predicting low and high crime rates -- a useful model, in other words. The intermediate categories cause much more difficulty though.

##K-Means Cluster Analysis

Cluster analysis is used to identify meaningful categories within a population. A meaningful category is a subgroup of the population whose members, in one way or another, behave similarly to other members of that same group, and dissimilarly from the members of the other subgroups.  
  
  As humans, we have a built-in pattern recognition firmware that helps us place the real-world phenomena that we encounter into pre-determined categories and respond to them accordingly. Therefore, the utility of cluster analysis might not be quite as intuitively obvious as that of a technique such as regression analysis or LDA. An example of a situation where cluster analysis may be useful is if, say, we discover a new species of deep-sea fish. At the beginning, we will have very little knowledge of the scale of variation -- morphological or otherwise -- present in the species. We might even have stumbled upon an entire new taxon of fish comprising more than one separate species! In cluster analysis, we would gather measurements on a set of characteristics from as large a sample of the population as possible -- say, weight, length, number of scales, tooth length, etc. -- and then use cluster analysis to see how many meaningful subgroups the population seems to divide into. After doing so, we might summon a board of biologists to decide whether these subgroups represent a set of sub-species of a single new species, or a set of more than one new species. Then we would proceed to give them names, descriptions, and so on.  
  
  In the present exercise, we will use cluster analysis to see how many meaningful group distinctions emerge among the different Massachusetts townships represented in our dataset. To do this, we need a version of the Boston dataset where _crim_ is back in numeric format, and everything has been scaled:  
```{r}
Boston.adulterated<-Boston #Back up the version in which crime is a factor.
Boston<-as.data.frame(scale(Boston.unscaled)) #Scale the original version of Boston and make sure the result is a data frame:
head(Boston)
```
Then we calculate the Euclidean distance between each town: 

```{r}
dist_eu<-dist(Boston)
```

Finally, we apply the clever algorithm shown in the Datacamp exercise in order to visually identify the optimal (most succinct and informative) number of of clusters (groups) into which the dataset divides:

```{r}
k_max <- 10;twcss <- sapply(1:k_max, function(k){kmeans(dist_eu, k)$tot.withinss});plot(1:k_max, twcss, type='b')
```
  
  Here we face the same dilemma as in stepwise regression: we can see that increasing the number of clusters decreases the number of residual variance, which is the goal. But more clusters means more categorizations, i.e., more complexity, more explaining to do, and more cognitive load. Therefore, we will abide by the rule of thumb -- the optimal number of clusters is the one that brings about the most radical change in residual variance -- and declare that 2 is the winning number. If we were seriously pursuing this line of investigation, we would now examine the clusters, identify their shared characteristics, and perhaps give them informative names or labels. But those are not the instructions. The instructions are to visualize the clusters. Hence now we visualize the clusters:

```{r}
km <-kmeans(dist_eu, centers = 2)
pairs(Boston, col = km$cluster)
```
  
  I don't get much out of this plot, either. There's too much stuff jammed into a tight space.
##Bonus
We'll now perform this analysis in reverse. We'll take 4 clusters -- this seems a relatively good number of clusters, judging from the second-to-last-plot -- and perform LDA on _Boston_ with the 4 clusters as target classes, in order to try and identify their common characteristics.  
  
```{r}
km4<-kmeans(dist_eu, centers = 4)
Boston$cluster<-km4$cluster
(ld4.4c<-lda(cluster ~ ., data=Boston))
```  
  
  
  
  Judging from the coefficients of the Linear Discrimimants (LDs), it appears that air pollution is by far most significant discriminating variable between the clusters, followed by property tax rate. However, it is not entirely clear to me how these coefficients are to be interpreted and applied. There are always K-1 of them, where K is the number of classes/clusters. Is it the case that LD1 differentiates the first cluster from the second, while LD2 differentiates the second cluster from the third? Hmmm... if the treatment of factors in regression analysis is any indication, then it seems likelier that one of the classes/clusters is used as the reference cluster, and the Linear Discriminants differentiate the other clusters from this reference cluster. This would explain why Proportion of Trace (=explanatory power) is highest for LD1 and progressively much smaller for the other LDs -- we already saw in the previous section that 2 is the optimal number of clusters, so the discriminant differentiating these two main clusters is by far the most significant one. Our current model has 4 clusters, and the drastically lower Proportion of Trace (explanatory power) of the other Linear Discriminants compared to the first one reflects the fact that adding new clusters after the first two yields diminishing returns. In other words, any additional classifications of the data that are done after the initial 2-way classification are much less helpful than the original 2-way classification. They do increase the overall predictive power, but it is debatable whether this improvement is enough to justify the resulting increase in model complexity.  
  
However, what exactly do the coefficients represent? How are they calculated? In linear and logistic regression, we can manually predict the value of the response variable using the coefficients. Can we similarly use the linear discriminant coefficients to predict which cluster a given data point belongs to? If so, how exactly is this done? I have yet to find an answer to this, neither by Googling nor in any of the course materials.  
    
At any rate, we can still use the Group Means data to improvise some hypotheses on the four clusters that have been identified:  

* Cluster 1: Lots of crime, lots of industry, lots of pollution, apartments are old and small, close proximity to central traffic routes and employment centers, and very few blacks in the population. Hmmm....this sounds like an old-fashioned white working-class neighborhood -- exactly the kind of place that is run by archetypcal gangsters in trench coats and fedoras. Think Al Capone, only in Boston instead of NY. In fact, if you've seen [this film](http://www.imdb.com/title/tt0407887/), then you know exactly the kind of place I'm envisioning.

* Cluster 2: The main difference between this one and Cluster 1 is in the dramatic reduction in crime and the fact that blacks are allowed in this neighborhood. Otherwise the two clusters seem to fit a similar profile. _Dis_ and _rad_ indicate that this is a type of suburb that tends to be a bit farther from downtown but is overall still quite conveniently located. Houses are still old and small, though not so old as in Cluster 1. Therefore, I conjecture that this cluster represents the 2nd layer, or ring, of the city that was built around the core once the core became too cramped to accomodate the growing population. These places are still relatively working-class but they are no longer run by gangsters like Cluster 1 is. The absence of racist gangsters enables the entry of non-white ethnic groups into the neighborhood.

* Cluster 3: These are suburbs proper -- sub-urban in the most authentic sense of the word. They represent the Outer Ring of a city -- relatively quiet and peaceful places with significantly less economical and industrial activity than in the previous two clusters. There is more space, so houses are bigger and tend to be new. People move here because of the increased peace and safety relative to downtown -- typically when they have their first children. The flipside of the increased peace and safety is the reduced access to many services and entertainment options -- you have to drive or use public transportation to access some of the shiniest and trendiest goodies. But couples with small children generally don't care  much for that kind of thing.

* Cluster 4: These are also outer suburbs, but they differ from Cluster 3 in one important respect: there are no families. These are districts constructed to provide affordable housing to newcomers who are young, single, and seeking career opportunities. Apartments are new and small, so they cost more than in Clusters 1 and 2 but significantly less than in Cluster 3.  
  
  Now it's time to draw one of those goofy arrow plots again -- biplot is what I think it's called. Sigh. Do I really have to?
```{r}
luokat <- Boston$cluster
plot(ld4.4c, dimen = 2, col=luokat, pch=luokat);lda.arrows(ld4.4c, myscale = 1.5)
```
  
  Try as I might, I just can't extract a whole lot of useful information from this plot. Maybe I could if there were fewer variables and the picture didn't get so cramped. But the instructions were what they were. Pretty much the only bits of data that I'm able to glean from this plot are these two:
  
1.Air pollution correlates with Cluster 1. Well that's to be expected of the core regions of the city, where the bulk of cars and businesses are.

2.Property Tax rates also correlate with Cluster 1. This one is harder to decipher. I imagine it might be because being close to downtown is valued highly, especially by businesses, so those properties are also taxed more heavily regardless of their condition.
  
  As the variables that are the most influential linear separators of the clusters, I can't answer that confidently since I'm still unclear on how to interpret the Linear Determinant coefficients. But if the Group Means are any indicator (and they darn well should be), then I'd say the biggest differentiators are _tax_, _indus_, _rad_ and _nox_. And I would also say that these variables likely reflect the the Downtown-Suburban dichotomy. This would also explain why the WCSS by Clusters plot earlier showed 2 to be the ideal number of clusters. It was probably showing us this same dichotomy that it was capturing, and our present 4-cluster model just adds a bit of extra nuance to it -- at the cost of increased model complexity. 
  